{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/kykim/bert-kor-base/resolve/main/vocab.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7876/90113723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbert_sim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_sim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zesky\\Desktop\\emotion_thearpy\\emotion_test\\bert_sim.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kykim/bert-kor-base\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kykim/bert-kor-base\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1761\u001b[0m                         \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1763\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munresolved_files\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1722\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m                     resolved_vocab_files[file_id] = cached_path(\n\u001b[0m\u001b[0;32m   1725\u001b[0m                         \u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m                         \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1919\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1921\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1922\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   2123\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2125\u001b[1;33m             \u001b[0m_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2126\u001b[0m             \u001b[0metag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X-Linked-Etag\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ETag\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2127\u001b[0m             \u001b[1;31m# We favor a custom header indicating the etag of the linked resource, and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[1;34m(request)\u001b[0m\n\u001b[0;32m   2050\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRevisionNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"404 Client Error: Revision Not Found for url: {request.url}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2052\u001b[1;33m     \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/kykim/bert-kor-base/resolve/main/vocab.txt"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import warnings\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from matplotlib import rc\n",
    "from bert_sim import word_sim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_data():\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        #self.wiki_model = Word2Vec.load('../ko/ko.bin')\n",
    "        if platform.system() == 'Windows':\n",
    "            self.font = 'Malgun Gothic'\n",
    "        else:\n",
    "            self.font = 'NanumGothic'\n",
    "        self.df['효능_Morps'] = self.df['효능'].map(lambda x: self.getMorps(x))\n",
    "        self.df['효능_NV'] = self.df['효능_Morps'].map(lambda x: self.getNV(x))\n",
    "        self.df['효능_N'] = self.df['효능_Morps'].map(lambda x: self.getN(x))\n",
    "  \n",
    "    def topn(self,n):\n",
    "        self.df['효능_count_topn_NV']  = self.df['효능_NV'].map(lambda x: self.counter(x,n)) \n",
    "        self.df['효능_count_topn_N']  = self.df['효능_N'].map(lambda x: self.counter(x,n)) \n",
    "\n",
    "    def result(self,idx, top , dis):\n",
    "        energy = ['정열', '활기', '기력', '생기', '활발','--']\n",
    "        recovery = ['낫' '치료', '휴식' ,'극복' , '재활', '--']\n",
    "        cycle = ['돌', '유지', '조절' , '공급' ,'생명', '--']\n",
    "        pury = ['진정' , '맑' , '없애' , '깨끗', '해소', '--']        \n",
    "        effs_dims_name = ['에너지', '회복', '순환', '정화']\n",
    "        effs_dims =  [energy, recovery, cycle , pury]\n",
    "        sNV = self.df['효능_NV'][idx]\n",
    "        \n",
    "        result = []\n",
    "        for dim in effs_dims:\n",
    "            ab = []\n",
    "            for sim_word in dim:\n",
    "                sub = []\n",
    "                for word in sNV:\n",
    "                    try:\n",
    "                        sub.append(word_sim(sim_word , word))\n",
    "                    except:\n",
    "                        sub.append(0)\n",
    "                ab.append(sub)\n",
    "                aab = np.array(ab).flatten().tolist()\n",
    "            aab = np.array(sorted(aab,reverse=True))[:top].mean()\n",
    "            result.append(aab)\n",
    "        \n",
    "        if dis == True:\n",
    "            from matplotlib import rc\n",
    "            plt.rc('font', family=self.font)\n",
    "\n",
    "            word = self.df.iloc[idx][0]\n",
    "            std = np.std(result)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.bar(np.arange(len(result)),result)\n",
    "            plt.title(f'{word}\\'s mapping for top {top} using 유사어')\n",
    "            plt.xticks(np.arange(len(effs_dims_name)), effs_dims_name)\n",
    "            plt.yticks(np.arange(0,1.2,0.2))\n",
    "            plt.show()\n",
    "#            print(f'{word}s std is {std}' )\n",
    "        return result\n",
    "        \n",
    "    def getMorps(self,sent):\n",
    "        return Okt().pos(sent , norm=False , stem = True) \n",
    "\n",
    "    def getNV(self, morps):\n",
    "        stop_words = \"하다 것 등 수 그 이 더욱 대해 예 머 매 내 수도 무엇 모든 이다 때 곧 식 또한 좀 꼭 번 해 과 바로 더욱 논 보이 난 \"\n",
    "        stop_words = set(stop_words.split(' '))\n",
    "\n",
    "        result = []\n",
    "        for item , morp in morps:\n",
    "            if morp == 'Verb' or morp == 'Noun': \n",
    "                result.append(item)\n",
    "        s_result = []\n",
    "        \n",
    "        for word in result:\n",
    "            if word not in stop_words:\n",
    "                s_result.append(word)\n",
    "        return s_result\n",
    "    \n",
    "    def getN(self, morps):\n",
    "        stop_words = \"하다 것 등 수 그 이 더욱 대해 예 머 매 내 수도 무엇 모든 이다 때 곧 식 또한 좀 꼭 번 해 과 바로 더욱 논 보이 난 \"\n",
    "\n",
    "        stop_words = set(stop_words.split(' '))\n",
    "\n",
    "        result = []\n",
    "        for item , morp in morps:\n",
    "            if morp == 'Noun': \n",
    "                result.append(item)\n",
    "        s_result = []\n",
    "        \n",
    "        for word in result:\n",
    "            if word not in stop_words:\n",
    "                s_result.append(word)\n",
    "        s_result = np.unique(s_result)\n",
    "        return s_result\n",
    "\n",
    "    def counter(self,sents, n):\n",
    "        return sorted(collections.Counter(sents),reverse=True)[0:n]\n",
    "\n",
    "    def get_heatmap(self, emotion_idx,figsize):\n",
    "        energy = ['정열', '활기', '기력', '생기', '활발','--']\n",
    "        recovery = ['낫', '치료', '휴식' ,'극복' , '재활', '--']\n",
    "        cycle = ['운반', '유지', '조절' , '공급' ,'생명', '--']\n",
    "        pury = ['진정' , '맑' , '없애' , '깨끗하', '해소', '--']        \n",
    "        all_dims = energy + recovery + cycle + pury\n",
    "        \n",
    "        plt.rc('font', family=self.font)\n",
    "        sents = self.df['효능'][emotion_idx]\n",
    "        print(sents)\n",
    "        \n",
    "        result = []\n",
    "        for word in sents:\n",
    "            sub = []\n",
    "            for dim_word in all_dims:\n",
    "                try:\n",
    "                    if dim_word == '--':\n",
    "                        sub.append(0)\n",
    "                        continue\n",
    "                    sub.append(word_sim(dim_word , word))\n",
    "                except:\n",
    "                    sub.append(0)\n",
    "            result.append(sub)\n",
    "\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        fig.set_facecolor('white')\n",
    "        em_name = self.df['감정'][emotion_idx]\n",
    "        plt.title(f'{em_name}에서 단어들간의 유사도 확인')\n",
    "        sns.heatmap(result,annot=True)\n",
    "        plt.xticks(np.arange(len(all_dims)),all_dims)\n",
    "        plt.yticks(np.arange(len(sents)),sents)\n",
    "        plt.show()\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>감정</th>\n",
       "      <th>효능</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>행복</td>\n",
       "      <td>향유 낙관적 느낌 생각 즐기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우울</td>\n",
       "      <td>행동 도움 생활 치료 명상</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>슬픔</td>\n",
       "      <td>사랑 도움 가족 시간 활동</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>열정</td>\n",
       "      <td>상기 집중 노력 몰입 휴식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>분노</td>\n",
       "      <td>정지 회피 긍정 마음 호흡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>두려움</td>\n",
       "      <td>직면 변화 대화 집중 용기</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    감정               효능\n",
       "0   행복  향유 낙관적 느낌 생각 즐기\n",
       "1   우울   행동 도움 생활 치료 명상\n",
       "2   슬픔   사랑 도움 가족 시간 활동\n",
       "3   열정   상기 집중 노력 몰입 휴식\n",
       "4   분노   정지 회피 긍정 마음 호흡\n",
       "5  두려움   직면 변화 대화 집중 용기"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정 데이터 확인\n",
    "em_data = pd.read_csv(\"../data/emotion_data_hand_craft.csv\")\n",
    "em_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 차원의 유사어와 각 감정의 키워드 사이의 유사도를 확인\n",
    "bert 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "향유 낙관적 느낌 생각 즐기\n",
      "행동 도움 생활 치료 명상\n",
      "사랑 도움 가족 시간 활동\n",
      "상기 집중 노력 몰입 휴식\n",
      "정지 회피 긍정 마음 호흡\n",
      "직면 변화 대화 집중 용기\n"
     ]
    }
   ],
   "source": [
    "hXYZ = check_data(em_data)\n",
    "for i in em_data.index:\n",
    "    hXYZ.get_heatmap(i, (20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29452f6d2151aa2870314585b3e91bb7a8843450c46b48401a824fa318bf3597"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
